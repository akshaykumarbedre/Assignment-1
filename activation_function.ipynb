{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.\n",
    "activation function is matheamtic function that used to activate a Perception when output is more thrn thoreod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.\n",
    "Sigmoid: \n",
    "ReLU(Rectified Linear Unit)\n",
    "tanH(Hyperbloic target)\n",
    "linear\n",
    "softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.\n",
    "it play key role in training  & performaase \n",
    "Sigmoid: it used for binanry classifation \n",
    "it has a tendency to saturate when the input is too large or too small, which can slow down the training process\n",
    "ReLU:because it is computationally efficient and does not suffer from saturation. However, it can lead to “dead” neurons that do not contribute to the output of the network.\n",
    "tanh:has a steeper gradient around zero. This can help speed up the training process compared to sigmoid, but it can also suffer from saturation.\n",
    "\n",
    "softmax: it multiclass classfiaction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.sigmoid activation function: it mathematic funsion  which  range from 0 to 1, his function sloap is The sigmoid function is defined as follows:\n",
    "\n",
    "σ(x)=1+/(e−x1​)\n",
    "where x is the input value.\n",
    "The main advantage of the sigmoid function is that it produces a smooth output that can be easily differentiated, which makes it suitable for use in gradient-based optimization algorithms such as backpropagation\n",
    "large & small value lead to saturation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5.(ReLU) activation function is mathe,mativ function range from 0 to infnity , \n",
    "y=max(0,x)\n",
    "but it is dead in negative or backward progartion \n",
    "sigmoid activation function: it mathematic funsion  which  range from 0 to 1, his function sloap is The sigmoid function is defined as follows:\n",
    "σ(x)=1+/(e−x1​)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.\n",
    "Compared to the sigmoid function, which maps any input value to a value between 0 and 1, the ReLU function has several advantages. One of the main advantages is that it does not suffer from saturation, which can speed up the training process. Additionally, it produces outputs that are zero-centered, which can make it easier to train deep neural networks.\n",
    "\n",
    "Another advantage of the ReLU function is that it is computationally efficient. This is because it only involves simple thresholding operations that can be easily implemented in hardware or software.\n",
    "\n",
    "Overall, the ReLU function has several benefits over the sigmoid function and is commonly used in modern deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7.leaky ReLU: it is mathematic funsion that postiavte side x=y, and when x is negeative y=0 as case of ReLU, but leaky ReLU when x is negative then x=y*0.01. when is backprogation it dont is dead state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.the softmax activation function is a mathematical function that maps a vector of real numbers to a probability distribution. It is commonly used in the output layer of a neural network for multi-class classification problems, where the goal is to predict the probability of each class given an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9.hyperbolic tangent (tanh) activation function: it is range -1 to 1  has a steeper gradient around zero\n",
    "the tanh function has a steeper gradient around zero, which can help speed up the training process compared to sigmoid. However, it can also suffer from saturation ."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.the purpose of forward propagation in a neural network is for to get output from input to multipaction of wigth @ input and giving output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.\n",
    "forward propagation implemented mathematically in a single-layer feedforward neural network\n",
    "consider the input as x1 and x2 which give output \n",
    "in feedforward neural network, x1*w1 +x2*w2 & based of actiavtion function it produe output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.\n",
    "The activation function introduces non-linearity into the output of a neuron, which is essential for neural networks to learn and perform complex tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.it used to calcuacte the output to  each neuron in the network, goal it to min the diff bw predict value & actiavlue values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5.\n",
    "purpose of applying a softmax function in the output layer during forward propagation is give the multiclass for classsifiactin problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6. purpose of backward propogatiob is to used the weigth of each nural in netword to minizire the error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7.input as x1, x2 ,,. weigth w1, w2\n",
    "in backward prapogation is to update the wigth to get min value bw actcale value & predit value \n",
    "\n",
    "w(new)x1=w(old)x1+alpha(diff(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8.\n",
    "The chain rule is a fundamental concept in calculus that allows us to find the derivative of a composite function. A composite function is a function that is made up of two or more functions. The chain rule tells us how to differentiate such functions by breaking them down into simpler parts.\n",
    "The chain rule states that if we have a composite function f(g(x)), then the derivative of f(g(x)) with respect to x is given by:\n",
    "(f(g(x)))′=f′(g(x))g′(x)\n",
    "application in backward propagation: it help to update the wigth of each neural netwrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9.\n",
    "Vanishing gradients: This occurs when the gradients become too small as they propagate backwards through the network, making it difficult to update the weights of the earlier layers. This can be addressed by using activation functions that do not saturate, such as ReLU, and by using weight initialization techniques that prevent the gradients from vanishing 1.\n",
    "Exploding gradients: This occurs when the gradients become too large as they propagate backwards through the network, making it difficult to update the weights of the earlier layers. This can be addressed by using gradient clipping, which involves scaling down the gradients if they exceed a certain threshold 1.\n",
    "Overfitting: This occurs when the model becomes too complex and starts to fit the training data too closely, resulting in poor generalization to new data. This can be addressed by using regularization techniques such as L1 or L2 regularization, dropout, or early stopping 21.\n",
    "Underfitting: This occurs when the model is too simple and fails to capture the underlying patterns in the data. This can be addressed by increasing the complexity of the model, adding more layers or neurons, or using a different architecture altogether 21."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
